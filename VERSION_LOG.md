# Version Log — GlyphDrift: Emergent Symbolic Language

Each version adds one layer to the system. All scores from 20 episodes,
seeds 42-61. Mean +/- 95% CI reported.

## Scoring Key

- **Episodes:** 20 per condition
- **Generations:** 1000 per episode (v0: 100 sampled, no evolution)
- **Seeds:** 42-61 (one per episode)
- **Max Shannon entropy:** log2(40) = 5.322 bits (40 glyphs, uniform)

---

## v0 — Glyph Atoms: The Primordial Soup

**System:** 40 Unicode glyphs (10 particles, 10 roots, 10 suffixes, 10 modifiers).
Population of N sequences of length L. No evolution — pure random generation
each generation with Zipf-weighted base distribution tempered by entropy parameter.
entropy=0.0 gives peaked Zipf (few glyphs dominate), entropy=1.0 gives uniform random.

| Metric | base (e=0.5) | high_entropy (e=0.9) | low_entropy (e=0.1) | large_pop (n=500) |
|--------|-------------|---------------------|--------------------|--------------------|
| shannon_entropy | 5.024 ± 0.014 | 5.281 ± 0.003 | 4.539 ± 0.024 | 5.032 ± 0.007 |
| diversity_ratio | 1.000 ± 0.000 | 1.000 ± 0.000 | 1.000 ± 0.000 | 1.000 ± 0.000 |
| unique_glyphs | 40.0 ± 0.0 | 40.0 ± 0.0 | 40.0 ± 0.0 | 40.0 ± 0.0 |

**Observations:**
- Entropy parameter works as designed: low entropy concentrates probability on
  high-rank glyphs (Shannon 4.54 = 85% of max), high entropy approaches uniform
  (5.28 = 99% of max).
- All 40 glyphs appear in every run because 1000 draws from 40 symbols guarantees
  full coverage even at low entropy. The difference is in FREQUENCY, not presence.
- Diversity ratio is 1.0 (all sequences unique) because without selection pressure,
  no convergence occurs. Random sampling produces no structure.
- Larger population (500 vs 100) gives tighter CIs but same Shannon entropy.
- These are the baselines. Evolution (v3) will break the diversity=1.0 ceiling
  and selection pressure should reduce Shannon entropy as vocabulary converges.

---

## v1 — Lexicon: Role-Based Phrase Structure

**Adds:** Lexicon class organizing glyphs by role (particle/root/suffix/modifier).
Phrase templates define role order: `[particle]-[root]-[suffix]` and 3 variants.
Sequences generated by sampling from role categories per template slot, padded
to target length with random-role glyphs.

| Metric | structured (e=0.5) | struct_high (e=0.9) | struct_low (e=0.1) |
|--------|-------------------|--------------------|--------------------|
| shannon_entropy | 5.175 ± 0.009 | 5.282 ± 0.004 | 4.936 ± 0.016 |
| template_adherence | 1.000 ± 0.000 | 1.000 ± 0.000 | 1.000 ± 0.000 |
| diversity_ratio | 1.000 ± 0.000 | 1.000 ± 0.000 | 1.000 ± 0.000 |

**Comparison to v0 (unstructured):**

| Entropy level | v0 Shannon | v1 Shannon | Change |
|---------------|-----------|-----------|--------|
| low (0.1) | 4.539 | 4.936 | **+8.7%** |
| mid (0.5) | 5.024 | 5.175 | **+3.0%** |
| high (0.9) | 5.281 | 5.282 | ~0% |

**Observations:**
- Template adherence is trivially 1.0 — all phrases are generated from templates.
  This metric becomes meaningful in v3+ when evolution can BREAK template structure.
- Structured generation **increases** entropy vs unstructured at low entropy (+8.7%).
  Mechanism: role-based templates force balanced sampling from all 4 categories (10
  glyphs each), preventing Zipf concentration on a few high-rank glyphs. Structure
  acts as a DIVERSIFIER, not a constraint — the opposite of what you'd expect.
- At high entropy (0.9), both converge to near-max (~5.28/5.32) because uniform
  random sampling overwhelms any structural effect.
- Still no sequence convergence (diversity=1.0). This changes with evolution (v3).

---

## v2 — Dialects: Divergent Vocabularies

**Adds:** 4 dialects (Council, Veydris, Xael, Masks) with overlapping glyph sets.
Each dialect gets 7 glyphs per role (out of 10), shifted by 2 indices per dialect —
giving ~30% overlap between adjacent dialects. Cross-dialect mixing borrows glyphs
from foreign dialects at a configurable rate. Population is evenly distributed across
dialects.

**Dialect structure:** 28 glyphs per dialect (7×4 roles), mean pairwise Jaccard distance = 0.522.

| Metric | four_dialects (mix=0.1) | high_mixing (mix=0.5) | isolated (mix=0.0) |
|--------|------------------------|----------------------|---------------------|
| shannon_entropy | 5.084 ± 0.010 | 5.100 ± 0.009 | 5.088 ± 0.010 |
| diversity_ratio | 1.000 ± 0.000 | 1.000 ± 0.000 | 1.000 ± 0.000 |
| unique_glyphs | 40.0 ± 0.0 | 40.0 ± 0.0 | 40.0 ± 0.0 |
| mean_jaccard | 0.522 ± 0.000 | 0.522 ± 0.000 | 0.522 ± 0.000 |
| mean_foreign_frac | 0.035 ± 0.000 | 0.175 ± 0.001 | 0.000 ± 0.000 |

**Comparison to v0 base (unstructured, e=0.5):**

| Metric | v0 base | v2 four_dialects | v2 isolated | Change (isolated vs v0) |
|--------|---------|-----------------|-------------|------------------------|
| shannon_entropy | 5.024 | 5.084 | 5.088 | **+1.3%** |

**Observations:**
- Dialects **increase** Shannon entropy vs unstructured (+1.3%), similar to v1's diversifying
  effect. Mechanism: each dialect samples from 28/40 glyphs with Zipf weighting, but the
  population spans 4 dialects, ensuring all 40 glyphs get used. Dialect partitioning acts
  as another form of forced diversification.
- Mixing rate has minimal effect on Shannon entropy (5.084 vs 5.100 vs 5.088). The
  population already spans all 4 dialects, so borrowing foreign glyphs doesn't change the
  global frequency distribution much.
- Foreign fraction scales linearly with mixing rate: 0% at mix=0.0, 3.5% at mix=0.1,
  17.5% at mix=0.5. The ~3.5x ratio (not 5x) is because some "foreign" glyphs overlap
  with the home dialect and don't count as foreign.
- Mean Jaccard distance is constant (0.522) — it measures dialect structure, which is
  fixed regardless of mixing. This confirms ~48% overlap between dialect pairs on average.
- All 40 unique glyphs appear in every run because the 4 dialects collectively cover the
  full alphabet.
- Still diversity=1.0 and no convergence — this changes with evolution (v3).
- The key insight: **both structure (v1) and partitioning (v2) increase diversity relative
  to unstructured random sampling.** Constraints don't constrain — they diversify.

---

## v3 — Evolution: Selection Pressure (THE KEY VERSION)

**Adds:** Tournament selection, bigram-coherence fitness function, substitution/insertion/deletion
mutation, uniform crossover. Population now EVOLVES — sequences with common bigram patterns
score higher and reproduce more. This is where random generation becomes an evolutionary system.

**Fitness:** sum of population-wide bigram frequencies for each sequence's bigrams.
Sequences containing patterns shared by others score higher — implicit selection for
shared structure without coding grammar rules.

| Metric | evolution_base | high_mutation | low_mutation | large_tournament | no_selection |
|--------|---------------|--------------|-------------|-----------------|-------------|
| | mut=0.1, t=3 | mut=0.3, t=3 | mut=0.02, t=3 | mut=0.1, t=7 | mut=0.1, t=1 |
| shannon_entropy | 1.814 ± 0.151 | 4.856 ± 0.040 | 0.255 ± 0.093 | 1.011 ± 0.050 | 5.199 ± 0.013 |
| diversity_ratio | 0.857 ± 0.018 | 1.000 ± 0.000 | 0.181 ± 0.017 | 0.672 ± 0.029 | 0.982 ± 0.005 |
| unique_glyphs | 39.1 ± 0.5 | 40.0 ± 0.0 | 16.1 ± 1.1 | 37.6 ± 0.6 | 40.0 ± 0.0 |
| unique_sequences | 85.7 ± 1.8 | 100.0 ± 0.0 | 18.1 ± 1.7 | 67.2 ± 2.9 | 98.2 ± 0.5 |
| mean_fitness | 3379 ± 306 | 63 ± 1 | 7129 ± 390 | 5143 ± 19 | 21 ± 0.04 |
| final_fitness | 3444 ± 340 | 64 ± 6 | 7283 ± 361 | 5263 ± 77 | 21 ± 0.6 |

**Comparison to v0 base (no evolution, e=0.5):**

| Metric | v0 base | v3 evolution_base | v3 low_mutation | v3 no_selection |
|--------|---------|------------------|----------------|----------------|
| shannon_entropy | 5.024 | **1.814 (-64%)** | **0.255 (-95%)** | 5.199 (+3.5%) |
| diversity_ratio | 1.000 | **0.857 (-14%)** | **0.181 (-82%)** | 0.982 (-1.8%) |
| unique_glyphs | 40.0 | 39.1 | **16.1 (-60%)** | 40.0 |

**Observations:**

1. **Evolution WORKS — vocabulary convergence is real.** v0-v2 were locked at diversity=1.0
   and Shannon ~5.0. Evolution shatters this: base evolution drops Shannon by 64% (5.02→1.81)
   and diversity by 14% (1.00→0.86). Low mutation goes extreme: 95% entropy reduction,
   82% diversity loss, vocabulary collapses to ~16 glyphs.

2. **Mutation rate controls the exploration-exploitation tradeoff:**
   - **Low (0.02):** Strongest convergence. Shannon=0.26, only 18 unique sequences out of 100.
     The population crystallizes around a few dominant bigram patterns. Fitness is highest
     (7283) because the population is nearly clonal — everyone shares the same bigrams.
   - **Base (0.1):** Balanced. Shannon=1.81, 86 unique sequences. Good convergence with
     enough diversity to keep exploring.
   - **High (0.3):** Mutation overwhelms selection. Shannon=4.86, diversity=1.0, all 40
     glyphs present. Indistinguishable from random — 30% per-glyph mutation destroys any
     structure selection builds. Fitness collapses to 63 (vs 3444 at base).

3. **Selection pressure amplifies convergence:** Large tournament (t=7 vs t=3) drops Shannon
   from 1.81 to 1.01 and diversity from 0.86 to 0.67. Stronger selection means fewer
   survivors, faster fixation. But fitness is higher (5143 vs 3379) — stronger selection
   is more efficient at concentrating the population on high-fitness patterns.

4. **No selection = null control.** Tournament size 1 (random reproduction) gives Shannon=5.20,
   diversity=0.98, fitness=21. Nearly identical to v0's random generation. This confirms
   that convergence requires selection pressure — mutation alone doesn't create structure.
   (The slight diversity drop from 1.0→0.98 is because crossover can occasionally produce
   duplicate sequences.)

5. **Fitness-diversity tradeoff is stark:** Low mutation achieves 7283 fitness but only 18
   unique sequences. High mutation has 100 unique sequences but fitness of 64. There is no
   free lunch — coherence costs diversity.

6. **Emergent vocabulary reduction.** At low mutation, 24 of 40 glyphs go extinct. The
   population naturally converges on a ~16-glyph "core vocabulary" without any explicit
   vocabulary constraint. This is genuine emergent specialization.

---

## v4 — Chaotic Drift: Logistic Map Dynamics

**Adds:** Mutation rate driven by logistic map `x_{n+1} = r * x_n * (1 - x_n)` instead
of constant. At r=3.9 (chaotic regime), mutation rate oscillates unpredictably — periods
of stability punctuated by bursts of high mutation. Optional entropy storms force x to 0.99
every N generations.

**Mutation dynamics:** `mutation_rate(t) = base_rate * x(t)` where x follows the logistic map.
Mean rate ≈ 0.06 (vs constant 0.1 in v3), so drift effectively halves the mutation budget.

| Metric | chaotic (r=3.9) | periodic (r=3.2) | stable (r=2.5) | storms (r=3.9, N=100) | v3 base (constant) |
|--------|----------------|-----------------|---------------|----------------------|-------------------|
| shannon_entropy | 0.858 ± 0.098 | 1.105 ± 0.217 | 0.764 ± 0.039 | 0.702 ± 0.092 | 1.814 ± 0.151 |
| diversity_ratio | 0.560 ± 0.028 | 0.628 ± 0.020 | 0.534 ± 0.024 | 0.447 ± 0.017 | 0.857 ± 0.018 |
| unique_glyphs | 35.0 ± 1.1 | 36.6 ± 0.7 | 33.6 ± 0.8 | 31.2 ± 1.0 | 39.1 ± 0.5 |
| unique_sequences | 55.9 ± 2.8 | 62.8 ± 2.0 | 53.4 ± 2.3 | 44.7 ± 1.7 | 85.7 ± 1.8 |
| mean_fitness | 5669 ± 338 | 5206 ± 527 | 5849 ± 26 | 5614 ± 335 | 3379 ± 306 |
| final_fitness | 6181 ± 420 | 5525 ± 571 | 5974 ± 116 | 5033 ± 327 | 3444 ± 340 |
| mean_mutation_rate | 0.059 | 0.066 | 0.060 | 0.060 | 0.100 (constant) |
| mutation_rate_var | 0.0009 | 0.0002 | ~0 | 0.0009 | 0 |

**Comparison to v3 evolution_base (constant mutation 0.1):**

| Metric | v3 constant | v4 chaotic | v4 storms | Change (chaotic vs v3) |
|--------|-----------|-----------|----------|----------------------|
| shannon_entropy | 1.814 | 0.858 | 0.702 | **-53%** |
| diversity_ratio | 0.857 | 0.560 | 0.447 | **-35%** |
| mean_fitness | 3379 | 5669 | 5614 | **+68%** |
| unique_glyphs | 39.1 | 35.0 | 31.2 | -10% |

**Observations:**

1. **Chaotic drift AMPLIFIES convergence.** All v4 conditions produce stronger convergence
   than v3's constant mutation. Shannon drops from 1.81 (v3) to 0.86 (chaotic) — a 53%
   further reduction. Fitness jumps 68% (3379→5669). The logistic map doesn't just add
   noise — it fundamentally changes the evolutionary dynamics.

2. **Mechanism: lower effective mutation.** Mean mutation rate is ~0.06 vs constant 0.1.
   The logistic map's average is ~0.59×base_rate, effectively halving the mutation budget.
   Less mutation = less disruption = faster convergence. But it's NOT just lower mutation —
   the chaotic TIMING matters too (see point 4).

3. **Entropy storms increase convergence (counterintuitive).** Storms produce the LOWEST
   Shannon (0.702) and diversity (0.447). Expected: storms inject chaos and slow convergence.
   Actual: storms create "punctuated equilibrium" — periodic disruptions force the population
   through bottlenecks, and the survivors share even more bigrams. Storms are PURGES, not
   just perturbations.

4. **Chaos vs stability vs periodicity:** Stable drift (r=2.5) converges slightly more than
   chaotic (Shannon 0.76 vs 0.86) — because stable mutation allows more consistent selection.
   But chaotic drift has higher fitness variance across runs (CI: ±338 vs ±26), meaning chaos
   creates more DIVERSE evolutionary outcomes across independent runs. Periodic (r=3.2) is
   in between on all measures, as expected.

5. **Vocabulary erosion continues.** Storms reduce vocabulary to 31.2 glyphs (22% loss),
   chaotic to 35, stable to 33.6. The combination of lower mutation + stronger convergence
   drives more glyphs to extinction.

6. **The r parameter controls predictability, not just rate.** All three r values produce
   similar mean mutation rates (~0.06), but variance differs by 1000x: stable=~0, periodic=0.0002,
   chaotic=0.0009. The difference is in WHEN mutation happens, not HOW MUCH.

---

## v5 — Grammar Emergence: Statistical Structure Detection

**Adds:** N-gram frequency tracking, chi-squared significance testing for emergent "grammar
rules" (bigrams/trigrams appearing above chance), mutual information between adjacent
positions, gzip compression ratio as structural complexity proxy.

**Grammar rule:** A bigram or trigram appearing significantly above expected frequency
(chi-squared, p < 0.05) with expected > 1.0. Expected frequency = N × P(a) × P(b)
under the null hypothesis of independent glyph positions.

| Metric | grammar_base | high_pressure | no_pressure | storms |
|--------|-------------|--------------|------------|--------|
| | drift+t=3 | drift+t=7 | t=1 (random) | drift+t=3+storms |
| shannon_entropy | 0.858 ± 0.098 | 0.700 ± 0.029 | 5.199 ± 0.013 | 0.702 ± 0.092 |
| diversity_ratio | 0.560 ± 0.028 | 0.510 ± 0.020 | 0.982 ± 0.005 | 0.447 ± 0.017 |
| sig_bigrams | **0.1 ± 0.2** | **0.0 ± 0.0** | **14.0 ± 1.9** | **0.15 ± 0.3** |
| sig_trigrams | 0.1 ± 0.2 | 0.0 ± 0.0 | 0.0 ± 0.0 | 0.1 ± 0.2 |
| mutual_info | 0.088 ± 0.053 | 0.040 ± 0.006 | **1.537 ± 0.028** | 0.080 ± 0.056 |
| compression_ratio | **0.073 ± 0.003** | **0.065 ± 0.002** | 0.296 ± 0.002 | **0.062 ± 0.002** |
| mean_fitness | 5669 ± 338 | 6233 ± 31 | 21 ± 0.04 | 5614 ± 335 |

**The Paradox: Selection DESTROYS measurable grammar while creating real structure.**

| Metric | No pressure | Base selection | High pressure |
|--------|------------|---------------|--------------|
| sig_bigrams | 14.0 | 0.1 | 0.0 |
| mutual_info | 1.54 | 0.09 | 0.04 |
| compression_ratio | 0.30 | 0.07 | 0.07 |

**Observations:**

1. **The central paradox of this project.** No-pressure (random reproduction) produces
   14 significant bigram rules and MI of 1.54 bits. Evolved populations (base, high pressure)
   produce ZERO significant bigrams and MI near zero. Evolution kills grammar by the
   chi-squared test's own definition.

2. **Why this happens — convergence kills variance.** The chi-squared test detects patterns
   that deviate from the null hypothesis (independent positions). In the no-pressure
   population, positions are approximately independent but with enough sampling noise
   to produce apparent patterns. In evolved populations, the population has CONVERGED —
   most sequences are nearly identical, so bigram frequencies are extremely skewed toward
   a few dominant patterns. But these dominant patterns are EXPECTED given the converged
   marginal frequencies — P(a,b) ≈ P(a)×P(b) because the same few glyphs dominate everywhere.
   The chi-squared test sees convergence as "consistent with the null," not as structure.

3. **Compression ratio tells the REAL story.** Evolved populations compress to 6-7% of raw
   size vs 30% for random. Gzip doesn't care about statistical tests — it finds
   repetition. Evolved populations are HIGHLY structured (repetitive bigrams, near-clonal
   sequences), but this structure registers as "uninteresting" to chi-squared because
   it's a trivial pattern (same sequence repeated).

4. **Mutual information confirms: convergence ≠ complexity.** MI measures statistical
   dependence between positions. Random populations have position-to-position variation
   that creates measurable MI. Converged populations have near-zero MI because knowing
   position i tells you nothing new about position i+1 — you already know both positions
   are dominated by the same 2-3 glyphs.

5. **Entropy storms don't help.** Storms produce similar grammar metrics (0.15 bigrams,
   0.08 MI) to base evolution. The perturbations disrupt structure but don't create the
   kind of intermediate-convergence regime where grammar would be detectable.

6. **The compression ratio hierarchy is inverted vs grammar rules:** Random (0.30) >
   Base (0.07) > High pressure (0.065) > Storms (0.062). More selection = more
   compressible = more repetitive. This is structure, just not the kind chi-squared tests
   are designed to detect.

7. **Implication for artificial life research:** Standard statistical tests for grammar
   emergence (chi-squared, MI) may be blind to the most important kind of emergent
   structure — convergent vocabularies and repetitive patterns. Compression-based metrics
   (Kolmogorov complexity proxies) may be better suited to detecting emergence in
   evolutionary systems. The "grammar" that evolves isn't syntactic rules — it's
   vocabulary convergence and sequence-level repetition.

---

## v6 — Non-Circular Fitness + Improved Grammar Detection

**Two fixes in one version:**

### 6a. Sliding Window Fitness

**Problem (v3-v5):** `bigram_coherence` computes bigram frequencies FROM the current
population, then scores each sequence against those same frequencies. Self-referential —
sequences reward the bigram patterns they help create.

**Fix:** Score generation t against generation t-1's bigram distribution. First generation
uses uniform prior (all bigrams count=1). Each subsequent generation scored against its
predecessor's bigrams. Breaks the self-referential feedback loop.

### 6b. Permutation-Based Grammar Detection

**Problem (v5):** Chi-squared uses `P(a)*P(b)` as expected frequency. In converged
populations, the marginals already encode the structure, so everything looks "expected."

**Fix:** Permutation test — shuffle glyph positions within each sequence (destroying
positional structure but preserving marginal frequencies). Compare observed bigram counts
against 100 shuffled versions. Also added position-specific entropy and Normalized
Compression Distance (NCD).

| Metric | sliding_base | sliding_high | no_select | circular_compare |
|--------|-------------|-------------|-----------|-----------------|
| | sliding+drift+t=3 | sliding+drift+t=7 | sliding+t=1 | circular+drift+t=3 |
| shannon_entropy | 0.824 ± 0.045 | 0.771 ± 0.136 | 5.199 ± 0.013 | 0.858 ± 0.098 |
| diversity_ratio | 0.563 ± 0.029 | 0.510 ± 0.020 | 0.982 ± 0.005 | 0.560 ± 0.028 |
| unique_glyphs | 35.0 ± 1.0 | 34.1 ± 1.1 | 40.0 ± 0.0 | 35.0 ± 1.1 |
| unique_sequences | 56.3 ± 2.9 | 50.9 ± 2.0 | 98.2 ± 0.5 | 55.9 ± 2.8 |
| mean_fitness | 5797 ± 16 | 5962 ± 490 | 15.9 ± 0.04 | 5669 ± 338 |
| final_fitness | 5644 ± 171 | 6073 ± 418 | 15.5 ± 0.4 | 6181 ± 420 |
| sig_bigrams (chi2) | 0.0 ± 0.0 | 0.15 ± 0.29 | 14.0 ± 1.9 | 0.1 ± 0.2 |
| mutual_info | 0.063 ± 0.008 | 0.111 ± 0.142 | 1.537 ± 0.028 | 0.088 ± 0.053 |
| compression_ratio | 0.073 ± 0.003 | 0.066 ± 0.002 | 0.296 ± 0.002 | 0.073 ± 0.003 |
| **perm_bigrams** | **0.55 ± 0.30** | **0.35 ± 0.59** | **70.2 ± 3.3** | **0.55 ± 0.36** |
| **position_entropy** | **0.646 ± 0.037** | **0.550 ± 0.024** | **4.284 ± 0.025** | **0.641 ± 0.037** |
| **ncd_vs_shuffled** | **0.745 ± 0.009** | **0.738 ± 0.022** | **0.870 ± 0.004** | **0.749 ± 0.015** |

**Sliding vs Circular (same selection pressure, t=3):**

| Metric | Circular (v5-style) | Sliding Window (v6) | Change |
|--------|-------------------|-------------------|--------|
| shannon_entropy | 0.858 | 0.824 | **-4%** |
| mean_fitness | 5669 | 5797 | **+2%** |
| final_fitness | 6181 | 5644 | **-9%** |
| fitness CI width | ±420 | ±171 | **-59% (more stable)** |

**Observations:**

1. **Sliding window produces slightly more convergence.** Shannon drops 4% (0.858→0.824).
   Without self-referential inflation, the fitness landscape is less "flat" — genuine
   pattern matching against the previous generation's distribution provides a cleaner
   selection signal.

2. **Fitness is MORE stable with sliding window.** Final fitness CI narrows dramatically
   (±420 → ±171). Circular fitness inflates scores through self-reference, creating
   artificial variance. Sliding window produces more consistent evolutionary trajectories.

3. **BUT: the grammar paradox persists.** Permutation test finds 0.55 significant bigrams
   in evolved populations (t=3) vs 70.2 in the no-selection control. This is the same
   pattern as v5's chi-squared result. The problem isn't circular fitness — it's that
   selection pressure itself drives convergence that destroys positional grammar.

4. **The permutation test IS working correctly** — the no-selection control shows 70
   permutation-significant bigrams where chi-squared only found 14. The permutation test
   is 5x more sensitive. It just has nothing to find in converged populations.

5. **Position-specific entropy reveals the convergence mechanism.** Evolved: 0.646 bits
   (out of max ~5.3). No-selection: 4.284 bits. Evolution doesn't just reduce which glyphs
   appear — it constrains which glyphs appear WHERE. Position entropy of 0.6 means each
   position is dominated by ~1-2 glyphs, creating a rigid "template" from convergence
   rather than design.

6. **NCD confirms structure exists.** Evolved populations have NCD ~0.74 vs shuffled version.
   Random populations: NCD ~0.87. Lower NCD in evolved populations means they're MORE
   similar to their shuffled versions — because the sequences are already so homogeneous
   that shuffling doesn't change much. The structure is "everywhere" (global convergence),
   not "somewhere" (positional grammar).

7. **The fundamental insight deepens:** What evolves isn't grammar (positional rules) — it's
   VOCABULARY (frequency convergence). The population converges on a small set of dominant
   glyphs at every position. This is compressible (ratio ~0.07), NCD-detectable, and
   entropically measurable, but it's not grammar in the linguistic sense. Getting genuine
   positional structure may require fitness functions that explicitly reward positional
   diversity, or multi-population schemes where different subgroups develop different
   "roles" for different positions.

---
